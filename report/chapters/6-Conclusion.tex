In this work we tried different structures and preprocessing to achieve the classification of some one word commands. We saw that for our models a convolution is needed to achieve good performances. The best model is the most complex CNN but closely followed but the LSTM with a convolution and a smaller CNN, which are both networks with under 250k parameters. The MFCC preprocessing is clearly the winner as expected. We achieve a much better accuracy than the one that we've been able to find but we have an additional label that has 100\% accuracy. Even without it though, the accuracy is better. During this work we've been through an idea called ensemble machine learning and even if we didn't demonstrated here we found some interesting results during live tests. Ensemble machine learning make more robust to input twists predictions if the networks that are combined do not learn the same thing. But since the idea here was to keep the footprint low, we decided not to use it.